{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input generator of FairTrade Extension Work. \n",
    "# Namely, generate a commitment that the broker should provide for each epoch\n",
    "# so that other nodes can verify.\n",
    "# desired format of commitment: \n",
    "#     1. a list of hash values, each value represents a traded dataset's hash\n",
    "#     2. a list of occurence time, each value represents the traded times of the corresponding dataset\n",
    "#     3. a list of occurence posistions, each value represents the which dataset is traded in that position\n",
    "# general process of the algorithm:\n",
    "#     1. decide the number of traded datasets and total transactions in this epoch\n",
    "#     2. generate the list of hash values\n",
    "#     3. run a loop, each time, randomly decide which data to be sold, append the dataset's index into position list\n",
    "#     4. meanwhile, add 1 in the corresponding occurence time list\n",
    "# output format of the algorithm:\n",
    "#     1. hash values -> data[]\n",
    "#     2. occurence time -> res[]\n",
    "#     3. occurence positions -> loc[]\n",
    "# ========================================\n",
    "# calculation needed for verification (see Commitment.sol for details): \n",
    "#         hash chain calculation -> use values in loc[] as index of data[]\n",
    "#         merkle root calculation -> each leaf consists of datasets' hashes (in data[]) and sold times (in res[])\n",
    "#         total sold times -> add up all values in res[] and compare with the size of loc[]\n",
    "# output: if occurence time not match: return false\n",
    "#         elif total occurence differs, return false\n",
    "#         else return true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"0x5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9\", \"0x6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b\", \"0xd4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35\", \"0x4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce\", \"0x4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328cb08b5531fcacdabf8a\", \"0xef2d127de37b942baad06145e54b0c619a1f22327b2ebbcfbec78f5564afe39d\", \"0xe7f6c011776e8db7cd330b54174fd76f7d0216b612387a5ffcfb81e6f0919683\", \"0x7902699be42c8a8e46fbbb4501726517e86b22c56a189f7625a6da49081b2451\", \"0x2c624232cdd221771294dfbb310aca000a0df6ac8b66b696d90ef06fdefb64a3\", \"0x19581e27de7ced00ff1ce50b2047e7a567c76b1cbaebabe5ef03f7c3017bb5b7\"] , [13, 8, 10, 13, 9, 9, 12, 10, 6, 10] , [7, 0, 0, 9, 7, 0, 8, 9, 5, 1, 5, 6, 8, 0, 2, 7, 0, 0, 3, 1, 3, 6, 4, 2, 1, 5, 6, 0, 9, 3, 9, 2, 1, 2, 1, 6, 1, 9, 4, 0, 6, 7, 1, 0, 5, 4, 4, 4, 6, 2, 4, 6, 4, 8, 8, 0, 0, 9, 9, 0, 3, 2, 3, 3, 3, 9, 6, 0, 7, 8, 4, 7, 3, 9, 5, 4, 5, 5, 7, 1, 6, 3, 5, 3, 6, 5, 3, 9, 2, 6, 3, 7, 6, 2, 3, 7, 2, 8, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "\n",
    "# determine how many datasets are sold and \n",
    "# how many transactions are executed in the epoch\n",
    "# just a random process\n",
    "data_num = 10 \n",
    "tx_num = 100\n",
    "\n",
    "# initialize the list of sold datasets' hashes\n",
    "data = []\n",
    "for _ in range(data_num):\n",
    "    m = str(_).encode()\n",
    "    msg = hashlib.sha256(m)\n",
    "    data.append('0x' + str(msg.hexdigest() ))\n",
    "\n",
    "# convert '' to \"\", arguments format requirements smart contract input\n",
    "lis = json.dumps(data)    \n",
    "    \n",
    "# list of each dataset's sold times (occurence time list)\n",
    "# and list of the exact transaction orders, e.g., which dataset is transacted\n",
    "# from the first transaction to the last one in this epoch\n",
    "res = [0 for _ in range(data_num)]\n",
    "loc = []\n",
    "\n",
    "for i in range(tx_num):\n",
    "    data_index = random.randrange(data_num) # randomly decide which dataset is sold\n",
    "    res[data_index] += 1 # occurence +1\n",
    "    loc.append(data_index) # dataset's index decided on this transaction\n",
    "\n",
    "print(lis, \",\", res, \",\",  loc) # just paste the output into await testCmt.validateCommitment(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"0x5feceb66ffc86f38d952786c6d696c79c2dbc239dd4e91b46729d73a27fb57e9\", \"0x6b86b273ff34fce19d6b804eff5a3f5747ada4eaa22f1d49c01e52ddb7875b4b\", \"0xd4735e3a265e16eee03f59718b9b5d03019c07d8b6c51f90da3a666eec13ab35\", \"0x4e07408562bedb8b60ce05c1decfe3ad16b72230967de01f640b7e4729b49fce\", \"0x4b227777d4dd1fc61c6f884f48641d02b4d121d3fd328cb08b5531fcacdabf8a\", \"0xef2d127de37b942baad06145e54b0c619a1f22327b2ebbcfbec78f5564afe39d\"] , [50, 50, 50, 50, 50, 50] , [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "\n",
    "# deterministic process\n",
    "# how many datasets are sold -> data_num\n",
    "# how many txs per epoch -> tx_num \n",
    "# how many times is each dataset traded -> tx_time\n",
    "data_num = 6 \n",
    "tx_num = 300\n",
    "tx_time = 50\n",
    "\n",
    "# initialize the list of sold datasets' hashes\n",
    "data = []\n",
    "for _ in range(data_num):\n",
    "    m = str(_).encode()\n",
    "    msg = hashlib.sha256(m)\n",
    "    data.append('0x' + str(msg.hexdigest() ))\n",
    "\n",
    "# convert '' to \"\", arguments format requirements smart contract input\n",
    "lis = json.dumps(data)    \n",
    "    \n",
    "# list of each dataset's sold times (occurence time list)\n",
    "# and list of the exact transaction orders, e.g., which dataset is transacted\n",
    "# from the first transaction to the last one in this epoch\n",
    "res = [tx_time for _ in range(data_num)]\n",
    "loc = []\n",
    "\n",
    "for i in range(int(tx_num/tx_time)):\n",
    "#     data_index = random.randrange(data_num) # randomly decide which dataset is sold\n",
    "#     res[data_index] += 1 # occurence +1\n",
    "#     loc.append(i) # dataset's index decided on this transaction\n",
    "#     loc.append(i)\n",
    "#     loc.append(i)\n",
    "    for _ in range(tx_time):\n",
    "        loc.append(i)\n",
    "\n",
    "print(lis, \",\", res, \",\",  loc) # just paste the output into await testCmt.validateCommitment(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
